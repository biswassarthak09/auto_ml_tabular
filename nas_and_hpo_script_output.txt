NAS and HPO script output:-

python meta_automl.py
Loaded dataset: brazilian_houses
Loaded dataset: superconductivity
Loaded dataset: bike_sharing_demand
Loaded dataset: yprop_4_1
Loaded dataset: wine_quality
Adding dataset: brazilian_houses
Shape: (4329, 11)
Added successfully. Total datasets: 1
Adding dataset: superconductivity
Shape: (8611, 81)
Added successfully. Total datasets: 2
Adding dataset: bike_sharing_demand
Shape: (7038, 11)
Added successfully. Total datasets: 3
Adding dataset: yprop_4_1
Shape: (7196, 62)
Added successfully. Total datasets: 4
Adding dataset: wine_quality
Shape: (3946, 12)
Added successfully. Total datasets: 5

Starting meta-learning optimization for regression task...
Number of datasets: 5
Optimizing for performance across ALL datasets...
Models being searched: ['random_forest', 'linear_regression', 'svr', 'knn', 'decision_tree', 'gradient_boosting', 'mlp']
[I 2025-07-14 18:27:19,660] A new study created in memory with name: no-name-ef18a2f9-3eaa-4468-8ac6-973886234279
[I 2025-07-14 18:29:25,516] Trial 0 finished with value: 0.5724310121081407 and parameters: {'model': 'gradient_boosting', 'n_estimators': 53, 'gb_learning_rate': 0.23917179916882758, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 2, 'subsample': 0.6487179770630183}. Best is trial 0 with value: 0.5724310121081407.
[I 2025-07-14 18:30:11,459] Trial 1 finished with value: 0.07656016323586057 and parameters: {'model': 'svr', 'C': 0.0072170824200985745, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 0 with value: 0.5724310121081407.
Best trial: 0. Best value: 0.572431:   2%|▉                                               | 2/100 [02:51<2:08:46, 78.85s/it]2025-07-14 18:30:11.464719: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2
2025-07-14 18:30:11.464740: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB
2025-07-14 18:30:11.464748: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB
2025-07-14 18:30:11.464765: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-07-14 18:30:11.464778: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-07-14 18:30:12.015941: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.
[I 2025-07-14 18:33:02,966] Trial 2 finished with value: 0.5643380496181659 and parameters: {'model': 'mlp', 'input_units': 202, 'n_layers': 2, 'hidden_units_0': 215, 'hidden_units_1': 185, 'activation': 'tanh', 'mlp_learning_rate': 0.0022509185096752397, 'dropout_rate': 0.47896220442601894, 'batch_norm': False, 'epochs': 89, 'batch_size': 128}. Best is trial 0 with value: 0.5724310121081407.
[I 2025-07-14 18:47:49,825] Trial 3 finished with value: -3539964.0172530496 and parameters: {'model': 'svr', 'C': 46.39830406166533, 'kernel': 'poly', 'gamma': 'scale', 'degree': 4}. Best is trial 0 with value: 0.5724310121081407.
[I 2025-07-14 18:48:13,088] Trial 4 finished with value: 0.5003878718921408 and parameters: {'model': 'knn', 'n_neighbors': 6, 'weights': 'uniform', 'algorithm': 'kd_tree', 'p': 3}. Best is trial 0 with value: 0.5724310121081407.
[I 2025-07-14 18:48:18,917] Trial 5 finished with value: 0.42580888345923545 and parameters: {'model': 'linear_regression', 'fit_intercept': True}. Best is trial 0 with value: 0.5724310121081407.
[I 2025-07-14 18:49:11,393] Trial 6 finished with value: 0.036353231674516304 and parameters: {'model': 'svr', 'C': 23.171750466177933, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 0 with value: 0.5724310121081407.
[I 2025-07-14 18:56:33,737] Trial 7 finished with value: 0.5979480048067841 and parameters: {'model': 'gradient_boosting', 'n_estimators': 162, 'gb_learning_rate': 0.2238814044558019, 'max_depth': 7, 'min_samples_split': 15, 'min_samples_leaf': 9, 'subsample': 0.9788660498128675}. Best is trial 7 with value: 0.5979480048067841.
[I 2025-07-14 18:56:50,893] Trial 8 finished with value: 0.5466844543542114 and parameters: {'model': 'knn', 'n_neighbors': 16, 'weights': 'distance', 'algorithm': 'auto', 'p': 3}. Best is trial 7 with value: 0.5979480048067841.
Best trial: 7. Best value: 0.597948:   9%|████▏                                          | 9/100 [29:31<4:16:12, 168.93s/it]WARNING:tensorflow:5 out of the last 20 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x354cbfd00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[I 2025-07-14 19:08:49,088] Trial 9 finished with value: 0.4848145437872321 and parameters: {'model': 'mlp', 'input_units': 191, 'n_layers': 2, 'hidden_units_0': 107, 'hidden_units_1': 144, 'activation': 'elu', 'mlp_learning_rate': 0.0005339620711491839, 'dropout_rate': 0.417873304567894, 'batch_norm': True, 'epochs': 100, 'batch_size': 64}. Best is trial 7 with value: 0.5979480048067841.
[I 2025-07-14 19:13:45,726] Trial 10 finished with value: 0.6157490476618844 and parameters: {'model': 'gradient_boosting', 'n_estimators': 189, 'gb_learning_rate': 0.07058255435945159, 'max_depth': 4, 'min_samples_split': 19, 'min_samples_leaf': 9, 'subsample': 0.9656806845498148}. Best is trial 10 with value: 0.6157490476618844.
[I 2025-07-14 19:17:44,343] Trial 11 finished with value: 0.5809876084966289 and parameters: {'model': 'gradient_boosting', 'n_estimators': 195, 'gb_learning_rate': 0.037328949893985885, 'max_depth': 3, 'min_samples_split': 19, 'min_samples_leaf': 10, 'subsample': 0.9904930553831295}. Best is trial 10 with value: 0.6157490476618844.
[I 2025-07-14 19:18:09,976] Trial 12 finished with value: 0.5103544745191316 and parameters: {'model': 'random_forest', 'n_estimators': 179, 'max_depth': 5, 'min_samples_split': 18, 'min_samples_leaf': 9, 'rf_max_features': 'log2'}. Best is trial 10 with value: 0.6157490476618844.
[I 2025-07-14 19:18:10,587] Trial 13 finished with value: 0.4706049006587756 and parameters: {'model': 'decision_tree', 'max_depth': 18, 'min_samples_split': 14, 'min_samples_leaf': 7, 'dt_max_features': 'sqrt'}. Best is trial 10 with value: 0.6157490476618844.
[I 2025-07-14 19:25:05,005] Trial 14 finished with value: 0.6227337777851816 and parameters: {'model': 'gradient_boosting', 'n_estimators': 149, 'gb_learning_rate': 0.11361359078518446, 'max_depth': 7, 'min_samples_split': 12, 'min_samples_leaf': 6, 'subsample': 0.9934778492168403}. Best is trial 14 with value: 0.6227337777851816.
[I 2025-07-14 19:30:20,441] Trial 15 finished with value: 0.6274232867538094 and parameters: {'model': 'gradient_boosting', 'n_estimators': 129, 'gb_learning_rate': 0.0787294161790394, 'max_depth': 7, 'min_samples_split': 7, 'min_samples_leaf': 5, 'subsample': 0.8913043794803426}. Best is trial 15 with value: 0.6274232867538094.
[I 2025-07-14 19:35:43,279] Trial 16 finished with value: 0.6176055643711558 and parameters: {'model': 'gradient_boosting', 'n_estimators': 119, 'gb_learning_rate': 0.126422738623506, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 4, 'subsample': 0.8478463696374473}. Best is trial 15 with value: 0.6274232867538094.
[I 2025-07-14 19:36:26,209] Trial 17 finished with value: 0.6021932904623335 and parameters: {'model': 'random_forest', 'n_estimators': 127, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 5, 'rf_max_features': 'sqrt'}. Best is trial 15 with value: 0.6274232867538094.
[I 2025-07-14 19:36:26,641] Trial 18 finished with value: 0.48354380284847587 and parameters: {'model': 'decision_tree', 'max_depth': 12, 'min_samples_split': 11, 'min_samples_leaf': 6, 'dt_max_features': 'log2'}. Best is trial 15 with value: 0.6274232867538094.
[I 2025-07-14 19:36:32,360] Trial 19 finished with value: -3552376.403918598 and parameters: {'model': 'linear_regression', 'fit_intercept': False}. Best is trial 15 with value: 0.6274232867538094.
[I 2025-07-14 19:40:59,179] Trial 20 finished with value: 0.6160700712807462 and parameters: {'model': 'gradient_boosting', 'n_estimators': 131, 'gb_learning_rate': 0.13249890725765134, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 2, 'subsample': 0.8409953219554228}. Best is trial 15 with value: 0.6274232867538094.
[I 2025-07-14 19:45:06,016] Trial 21 finished with value: 0.6193401672294884 and parameters: {'model': 'gradient_boosting', 'n_estimators': 94, 'gb_learning_rate': 0.12066174297337982, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 5, 'subsample': 0.8375167946199392}. Best is trial 15 with value: 0.6274232867538094.
[I 2025-07-14 19:49:17,735] Trial 22 finished with value: 0.6270746676917509 and parameters: {'model': 'gradient_boosting', 'n_estimators': 93, 'gb_learning_rate': 0.09222675387715974, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 4, 'subsample': 0.8655328219986809}. Best is trial 15 with value: 0.6274232867538094.
[I 2025-07-14 19:53:03,834] Trial 23 finished with value: 0.6296378594465365 and parameters: {'model': 'gradient_boosting', 'n_estimators': 92, 'gb_learning_rate': 0.07188868938434995, 'max_depth': 7, 'min_samples_split': 7, 'min_samples_leaf': 4, 'subsample': 0.9031432499672561}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 19:56:51,212] Trial 24 finished with value: 0.5379150405919828 and parameters: {'model': 'gradient_boosting', 'n_estimators': 87, 'gb_learning_rate': 0.012200648847639273, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 3, 'subsample': 0.8817683720079165}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:00:11,042] Trial 25 finished with value: 0.6263894892535248 and parameters: {'model': 'gradient_boosting', 'n_estimators': 92, 'gb_learning_rate': 0.06901641352126829, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 4, 'subsample': 0.913435965453582}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:03:01,903] Trial 26 finished with value: 0.6236740830804662 and parameters: {'model': 'gradient_boosting', 'n_estimators': 69, 'gb_learning_rate': 0.07537210867038552, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 1, 'subsample': 0.7289269844462731}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:03:06,109] Trial 27 finished with value: 0.526513586234716 and parameters: {'model': 'knn', 'n_neighbors': 20, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 1}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:06:03,479] Trial 28 finished with value: -7.2683320867681145 and parameters: {'model': 'mlp', 'input_units': 71, 'n_layers': 1, 'hidden_units_0': 36, 'activation': 'relu', 'mlp_learning_rate': 0.00016750150757302752, 'dropout_rate': 0.007805360904158454, 'batch_norm': True, 'epochs': 20, 'batch_size': 32}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:11:07,977] Trial 29 finished with value: 0.6118300057584543 and parameters: {'model': 'gradient_boosting', 'n_estimators': 109, 'gb_learning_rate': 0.16384249100851622, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 4, 'subsample': 0.7750304499175504}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:11:26,705] Trial 30 finished with value: 0.5859094200748058 and parameters: {'model': 'random_forest', 'n_estimators': 78, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 7, 'rf_max_features': 'log2'}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:15:05,059] Trial 31 finished with value: 0.6263630780654014 and parameters: {'model': 'gradient_boosting', 'n_estimators': 101, 'gb_learning_rate': 0.06829170034452439, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 4, 'subsample': 0.9108239333330382}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:19:03,539] Trial 32 finished with value: 0.6252373093280389 and parameters: {'model': 'gradient_boosting', 'n_estimators': 110, 'gb_learning_rate': 0.08124167992844158, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 3, 'subsample': 0.9153225940866747}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:21:11,887] Trial 33 finished with value: 0.6074520132107222 and parameters: {'model': 'gradient_boosting', 'n_estimators': 71, 'gb_learning_rate': 0.052781957321011194, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 4, 'subsample': 0.9145935449140353}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:21:12,308] Trial 34 finished with value: 0.47623737175837977 and parameters: {'model': 'decision_tree', 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 3, 'dt_max_features': 'log2'}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:21:46,222] Trial 35 finished with value: 0.2302619700979075 and parameters: {'model': 'svr', 'C': 0.0015239544565771182, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:21:51,967] Trial 36 finished with value: -3552376.403918598 and parameters: {'model': 'linear_regression', 'fit_intercept': False}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:27:34,128] Trial 37 finished with value: 0.6246856595003869 and parameters: {'model': 'gradient_boosting', 'n_estimators': 140, 'gb_learning_rate': 0.09095589467361047, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 5, 'subsample': 0.8832720975321132}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:32:51,054] Trial 38 finished with value: 0.5083592882724455 and parameters: {'model': 'mlp', 'input_units': 70, 'n_layers': 3, 'hidden_units_0': 253, 'hidden_units_1': 33, 'hidden_units_2': 130, 'activation': 'tanh', 'mlp_learning_rate': 0.008927662599631259, 'dropout_rate': 0.1158936024009532, 'batch_norm': False, 'epochs': 37, 'batch_size': 32}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:33:32,231] Trial 39 finished with value: -8.274549373659937 and parameters: {'model': 'svr', 'C': 0.28150260130757715, 'kernel': 'poly', 'gamma': 'scale', 'degree': 2}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:33:33,831] Trial 40 finished with value: 0.4985587227430037 and parameters: {'model': 'knn', 'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'brute', 'p': 1}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:37:03,409] Trial 41 finished with value: 0.6179764898516036 and parameters: {'model': 'gradient_boosting', 'n_estimators': 98, 'gb_learning_rate': 0.0360020279269076, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 4, 'subsample': 0.9221120665283115}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:40:23,449] Trial 42 finished with value: 0.6208206844825457 and parameters: {'model': 'gradient_boosting', 'n_estimators': 107, 'gb_learning_rate': 0.09761282586337774, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 4, 'subsample': 0.9293498536593927}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:53:04,668] Trial 43 finished with value: 0.622779122146038 and parameters: {'model': 'gradient_boosting', 'n_estimators': 84, 'gb_learning_rate': 0.05591495075234609, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 5, 'subsample': 0.879317090473781}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 20:56:55,521] Trial 44 finished with value: 0.5696409157372314 and parameters: {'model': 'gradient_boosting', 'n_estimators': 94, 'gb_learning_rate': 0.015106975573970192, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 3, 'subsample': 0.9454365453752012}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:02:16,807] Trial 45 finished with value: 0.6153637188341553 and parameters: {'model': 'gradient_boosting', 'n_estimators': 116, 'gb_learning_rate': 0.15148243964197716, 'max_depth': 8, 'min_samples_split': 12, 'min_samples_leaf': 6, 'subsample': 0.8830448269191757}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:02:22,786] Trial 46 finished with value: 0.42580888345923545 and parameters: {'model': 'linear_regression', 'fit_intercept': True}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:05:27,821] Trial 47 finished with value: 0.6266143556004002 and parameters: {'model': 'gradient_boosting', 'n_estimators': 98, 'gb_learning_rate': 0.06352689799792027, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 4, 'subsample': 0.7997363771961488}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:07:38,264] Trial 48 finished with value: 0.6262731335598889 and parameters: {'model': 'gradient_boosting', 'n_estimators': 60, 'gb_learning_rate': 0.09762619814981019, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 3, 'subsample': 0.8002163703158557}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:08:10,396] Trial 49 finished with value: 0.6177066009167532 and parameters: {'model': 'random_forest', 'n_estimators': 82, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 2, 'rf_max_features': 'sqrt'}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:08:51,920] Trial 50 finished with value: -0.7485432732095065 and parameters: {'model': 'svr', 'C': 0.3762213236276386, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:12:04,353] Trial 51 finished with value: 0.6238104605683565 and parameters: {'model': 'gradient_boosting', 'n_estimators': 102, 'gb_learning_rate': 0.058954700772830985, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 4, 'subsample': 0.7906726701841482}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:15:08,322] Trial 52 finished with value: 0.6224034544796117 and parameters: {'model': 'gradient_boosting', 'n_estimators': 92, 'gb_learning_rate': 0.046760741569858835, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 4, 'subsample': 0.8613987250283138}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:17:52,545] Trial 53 finished with value: 0.6203697995812036 and parameters: {'model': 'gradient_boosting', 'n_estimators': 100, 'gb_learning_rate': 0.07557974414810596, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 4, 'subsample': 0.8131155342745993}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:22:43,685] Trial 54 finished with value: 0.6247687763522096 and parameters: {'model': 'gradient_boosting', 'n_estimators': 117, 'gb_learning_rate': 0.09443172899711941, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 5, 'subsample': 0.902139216379175}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:22:43,968] Trial 55 finished with value: 0.42161192391496904 and parameters: {'model': 'decision_tree', 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 5, 'dt_max_features': 'sqrt'}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:25:04,087] Trial 56 finished with value: 0.6049322388388999 and parameters: {'model': 'gradient_boosting', 'n_estimators': 79, 'gb_learning_rate': 0.0321308383237335, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 3, 'subsample': 0.7582019097963713}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:29:40,990] Trial 57 finished with value: -201243903.48105693 and parameters: {'model': 'mlp', 'input_units': 252, 'n_layers': 3, 'hidden_units_0': 136, 'hidden_units_1': 245, 'hidden_units_2': 253, 'activation': 'relu', 'mlp_learning_rate': 0.006743272737493256, 'dropout_rate': 0.2739151743089613, 'batch_norm': False, 'epochs': 65, 'batch_size': 64}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:29:41,369] Trial 58 finished with value: 0.5182204490917669 and parameters: {'model': 'knn', 'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 23 with value: 0.6296378594465365.
[I 2025-07-14 21:36:09,132] Trial 59 finished with value: 0.631595860273269 and parameters: {'model': 'gradient_boosting', 'n_estimators': 133, 'gb_learning_rate': 0.06862154115380056, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 6, 'subsample': 0.9493694340426639}. Best is trial 59 with value: 0.631595860273269.
[I 2025-07-14 21:43:01,007] Trial 60 finished with value: 0.5804168252421165 and parameters: {'model': 'gradient_boosting', 'n_estimators': 138, 'gb_learning_rate': 0.29151758417692064, 'max_depth': 8, 'min_samples_split': 11, 'min_samples_leaf': 7, 'subsample': 0.9460626534853364}. Best is trial 59 with value: 0.631595860273269.
[I 2025-07-14 21:49:29,702] Trial 61 finished with value: 0.6326781482693665 and parameters: {'model': 'gradient_boosting', 'n_estimators': 151, 'gb_learning_rate': 0.059203552762088225, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 6, 'subsample': 0.9435489880280641}. Best is trial 61 with value: 0.6326781482693665.
[I 2025-07-14 21:56:02,299] Trial 62 finished with value: 0.6321814810765789 and parameters: {'model': 'gradient_boosting', 'n_estimators': 151, 'gb_learning_rate': 0.062335492157744254, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 6, 'subsample': 0.9556526709197479}. Best is trial 61 with value: 0.6326781482693665.
[I 2025-07-14 22:02:52,367] Trial 63 finished with value: 0.6315125447871653 and parameters: {'model': 'gradient_boosting', 'n_estimators': 160, 'gb_learning_rate': 0.04721913147904301, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 6, 'subsample': 0.9499795871571932}. Best is trial 61 with value: 0.6326781482693665.
[I 2025-07-14 22:09:52,538] Trial 64 finished with value: 0.632195040705212 and parameters: {'model': 'gradient_boosting', 'n_estimators': 162, 'gb_learning_rate': 0.043244188663653926, 'max_depth': 7, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.962211160519084}. Best is trial 61 with value: 0.6326781482693665.
[I 2025-07-14 22:16:38,658] Trial 65 finished with value: 0.6311786953386714 and parameters: {'model': 'gradient_boosting', 'n_estimators': 161, 'gb_learning_rate': 0.030331865714068533, 'max_depth': 7, 'min_samples_split': 14, 'min_samples_leaf': 6, 'subsample': 0.9497879433773558}. Best is trial 61 with value: 0.6326781482693665.
[I 2025-07-14 22:23:47,474] Trial 66 finished with value: 0.6316852531794142 and parameters: {'model': 'gradient_boosting', 'n_estimators': 163, 'gb_learning_rate': 0.031136574656508126, 'max_depth': 7, 'min_samples_split': 15, 'min_samples_leaf': 8, 'subsample': 0.9675159460326571}. Best is trial 61 with value: 0.6326781482693665.
[I 2025-07-14 22:30:57,415] Trial 67 finished with value: 0.6301437966361377 and parameters: {'model': 'gradient_boosting', 'n_estimators': 166, 'gb_learning_rate': 0.026773400125147635, 'max_depth': 7, 'min_samples_split': 15, 'min_samples_leaf': 8, 'subsample': 0.9726001720406044}. Best is trial 61 with value: 0.6326781482693665.
[I 2025-07-14 22:31:03,326] Trial 68 finished with value: -3552376.403918598 and parameters: {'model': 'linear_regression', 'fit_intercept': False}. Best is trial 61 with value: 0.6326781482693665.
[I 2025-07-14 22:31:49,422] Trial 69 finished with value: 0.5793436523027868 and parameters: {'model': 'random_forest', 'n_estimators': 158, 'max_depth': 9, 'min_samples_split': 14, 'min_samples_leaf': 6, 'rf_max_features': 'sqrt'}. Best is trial 61 with value: 0.6326781482693665.
[I 2025-07-14 22:40:22,266] Trial 70 finished with value: 0.6347132148840355 and parameters: {'model': 'gradient_boosting', 'n_estimators': 175, 'gb_learning_rate': 0.04160763244965771, 'max_depth': 8, 'min_samples_split': 16, 'min_samples_leaf': 8, 'subsample': 0.9567507406672465}. Best is trial 70 with value: 0.6347132148840355.
[I 2025-07-14 22:49:01,884] Trial 71 finished with value: 0.6329413432966471 and parameters: {'model': 'gradient_boosting', 'n_estimators': 176, 'gb_learning_rate': 0.04538375790177626, 'max_depth': 8, 'min_samples_split': 16, 'min_samples_leaf': 8, 'subsample': 0.9574212408221479}. Best is trial 70 with value: 0.6347132148840355.
[I 2025-07-14 22:58:03,774] Trial 72 finished with value: 0.6298638997843774 and parameters: {'model': 'gradient_boosting', 'n_estimators': 177, 'gb_learning_rate': 0.04243638165598004, 'max_depth': 8, 'min_samples_split': 17, 'min_samples_leaf': 8, 'subsample': 0.9985067228174163}. Best is trial 70 with value: 0.6347132148840355.
[I 2025-07-14 23:06:35,244] Trial 73 finished with value: 0.6335667834587925 and parameters: {'model': 'gradient_boosting', 'n_estimators': 173, 'gb_learning_rate': 0.0465692715244073, 'max_depth': 8, 'min_samples_split': 16, 'min_samples_leaf': 8, 'subsample': 0.9642825972612921}. Best is trial 70 with value: 0.6347132148840355.
[I 2025-07-14 23:14:52,650] Trial 74 finished with value: 0.6315190870068231 and parameters: {'model': 'gradient_boosting', 'n_estimators': 175, 'gb_learning_rate': 0.021423262530591437, 'max_depth': 8, 'min_samples_split': 16, 'min_samples_leaf': 8, 'subsample': 0.9769570368636745}. Best is trial 70 with value: 0.6347132148840355.
[I 2025-07-14 23:14:53,137] Trial 75 finished with value: 0.49666863441641 and parameters: {'model': 'decision_tree', 'max_depth': 10, 'min_samples_split': 18, 'min_samples_leaf': 10, 'dt_max_features': 'sqrt'}. Best is trial 70 with value: 0.6347132148840355.
[I 2025-07-14 23:24:53,712] Trial 76 finished with value: 0.6333222794036518 and parameters: {'model': 'gradient_boosting', 'n_estimators': 184, 'gb_learning_rate': 0.04355009841717364, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 9, 'subsample': 0.9652083517805846}. Best is trial 70 with value: 0.6347132148840355.
[I 2025-07-14 23:35:31,952] Trial 77 finished with value: 0.6349931107854472 and parameters: {'model': 'gradient_boosting', 'n_estimators': 195, 'gb_learning_rate': 0.0412780920994139, 'max_depth': 9, 'min_samples_split': 16, 'min_samples_leaf': 9, 'subsample': 0.969206550135477}. Best is trial 77 with value: 0.6349931107854472.
Best trial: 77. Best value: 0.634993:  78%|█████████████████████████████████▌         | 78/100 [5:08:12<2:54:40, 476.39s/it]WARNING:tensorflow:5 out of the last 21 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3f8ecab90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[I 2025-07-14 23:38:42,794] Trial 78 finished with value: -5.626022858339932 and parameters: {'model': 'mlp', 'input_units': 134, 'n_layers': 1, 'hidden_units_0': 53, 'activation': 'elu', 'mlp_learning_rate': 0.00012708332310907796, 'dropout_rate': 0.2920313001456545, 'batch_norm': True, 'epochs': 64, 'batch_size': 128}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-14 23:38:46,079] Trial 79 finished with value: 0.5559072088252415 and parameters: {'model': 'knn', 'n_neighbors': 12, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-14 23:50:18,408] Trial 80 finished with value: 0.5965220506673031 and parameters: {'model': 'gradient_boosting', 'n_estimators': 199, 'gb_learning_rate': 0.19738075156649237, 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 9, 'subsample': 0.9830675707138334}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 00:00:31,422] Trial 81 finished with value: 0.6340596507249789 and parameters: {'model': 'gradient_boosting', 'n_estimators': 189, 'gb_learning_rate': 0.04342469576384948, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 8, 'subsample': 0.9608272859261999}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 00:10:41,649] Trial 82 finished with value: 0.6327127251027499 and parameters: {'model': 'gradient_boosting', 'n_estimators': 188, 'gb_learning_rate': 0.043309205038452786, 'max_depth': 9, 'min_samples_split': 16, 'min_samples_leaf': 10, 'subsample': 0.9652568657888919}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 00:20:26,467] Trial 83 finished with value: 0.6329141130738598 and parameters: {'model': 'gradient_boosting', 'n_estimators': 186, 'gb_learning_rate': 0.044021371864847045, 'max_depth': 9, 'min_samples_split': 16, 'min_samples_leaf': 9, 'subsample': 0.9335381322207034}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 00:31:00,083] Trial 84 finished with value: 0.6303503250752932 and parameters: {'model': 'gradient_boosting', 'n_estimators': 188, 'gb_learning_rate': 0.04829323016049765, 'max_depth': 9, 'min_samples_split': 16, 'min_samples_leaf': 9, 'subsample': 0.9835950033248988}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 00:32:10,732] Trial 85 finished with value: -2741615.726972925 and parameters: {'model': 'svr', 'C': 0.6773912527655545, 'kernel': 'poly', 'gamma': 'scale', 'degree': 5}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 00:38:05,518] Trial 86 finished with value: 0.6242301519718635 and parameters: {'model': 'gradient_boosting', 'n_estimators': 187, 'gb_learning_rate': 0.013196744432289461, 'max_depth': 9, 'min_samples_split': 20, 'min_samples_leaf': 9, 'subsample': 0.6013216724592377}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 00:47:40,424] Trial 87 finished with value: 0.6331655686480878 and parameters: {'model': 'gradient_boosting', 'n_estimators': 183, 'gb_learning_rate': 0.041042383340759755, 'max_depth': 9, 'min_samples_split': 16, 'min_samples_leaf': 10, 'subsample': 0.9320723313808075}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 00:57:56,804] Trial 88 finished with value: 0.6297090862477668 and parameters: {'model': 'gradient_boosting', 'n_estimators': 183, 'gb_learning_rate': 0.03925419339340399, 'max_depth': 9, 'min_samples_split': 16, 'min_samples_leaf': 10, 'subsample': 0.9979136467839784}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 00:58:02,566] Trial 89 finished with value: 0.42580888345923545 and parameters: {'model': 'linear_regression', 'fit_intercept': True}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 01:07:46,801] Trial 90 finished with value: 0.6343330290948196 and parameters: {'model': 'gradient_boosting', 'n_estimators': 193, 'gb_learning_rate': 0.021883755227291527, 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 10, 'subsample': 0.9347702650890954}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 01:17:28,087] Trial 91 finished with value: 0.6344751889336944 and parameters: {'model': 'gradient_boosting', 'n_estimators': 193, 'gb_learning_rate': 0.022528986142473523, 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 10, 'subsample': 0.9344087485051797}. Best is trial 77 with value: 0.6349931107854472.
[I 2025-07-15 01:27:09,371] Trial 92 finished with value: 0.63507707324949 and parameters: {'model': 'gradient_boosting', 'n_estimators': 194, 'gb_learning_rate': 0.023404376456702455, 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 10, 'subsample': 0.9336853132638502}. Best is trial 92 with value: 0.63507707324949.
[I 2025-07-15 01:37:31,691] Trial 93 finished with value: 0.6361404608550759 and parameters: {'model': 'gradient_boosting', 'n_estimators': 194, 'gb_learning_rate': 0.022066874983270316, 'max_depth': 10, 'min_samples_split': 18, 'min_samples_leaf': 10, 'subsample': 0.9296908181849177}. Best is trial 93 with value: 0.6361404608550759.
[I 2025-07-15 01:47:33,832] Trial 94 finished with value: 0.6218934773194706 and parameters: {'model': 'gradient_boosting', 'n_estimators': 196, 'gb_learning_rate': 0.010918063070650422, 'max_depth': 10, 'min_samples_split': 18, 'min_samples_leaf': 10, 'subsample': 0.9239443211490042}. Best is trial 93 with value: 0.6361404608550759.
[I 2025-07-15 01:48:17,351] Trial 95 finished with value: 0.5803898727326354 and parameters: {'model': 'random_forest', 'n_estimators': 193, 'max_depth': 10, 'min_samples_split': 17, 'min_samples_leaf': 10, 'rf_max_features': 'log2'}. Best is trial 93 with value: 0.6361404608550759.
[I 2025-07-15 01:57:39,379] Trial 96 finished with value: 0.6354927252183371 and parameters: {'model': 'gradient_boosting', 'n_estimators': 193, 'gb_learning_rate': 0.02371236796316698, 'max_depth': 9, 'min_samples_split': 18, 'min_samples_leaf': 10, 'subsample': 0.9040484371722003}. Best is trial 93 with value: 0.6361404608550759.
[I 2025-07-15 02:07:42,190] Trial 97 finished with value: 0.6371595622366669 and parameters: {'model': 'gradient_boosting', 'n_estimators': 193, 'gb_learning_rate': 0.020993680984793886, 'max_depth': 10, 'min_samples_split': 19, 'min_samples_leaf': 9, 'subsample': 0.9043349706436498}. Best is trial 97 with value: 0.6371595622366669.
[I 2025-07-15 02:07:42,592] Trial 98 finished with value: 0.49909042683409727 and parameters: {'model': 'decision_tree', 'max_depth': 11, 'min_samples_split': 19, 'min_samples_leaf': 9, 'dt_max_features': 'log2'}. Best is trial 97 with value: 0.6371595622366669.
[I 2025-07-15 02:17:40,638] Trial 99 finished with value: 0.6360203088372782 and parameters: {'model': 'gradient_boosting', 'n_estimators': 193, 'gb_learning_rate': 0.019688019549835938, 'max_depth': 10, 'min_samples_split': 19, 'min_samples_leaf': 10, 'subsample': 0.8982143933093434}. Best is trial 97 with value: 0.6371595622366669.
Best trial: 97. Best value: 0.63716: 100%|█████████████████████████████████████████████| 100/100 [7:50:20<00:00, 282.21s/it]

Meta-learning optimization completed!
Best model: gradient_boosting
Best average score across datasets: 0.6372
Best parameters: {'n_estimators': 193, 'max_depth': 10, 'min_samples_split': 19, 'min_samples_leaf': 9, 'subsample': 0.9043349706436498, 'learning_rate': 0.020993680984793886}

Test Results:

brazilian_houses:
  mse: 0.0271
  r2: 0.9568

superconductivity:
  mse: 20.1573
  r2: 0.9823

bike_sharing_demand:
  mse: 1205.8090
  r2: 0.9625

yprop_4_1:
  mse: 0.0001
  r2: 0.8285

wine_quality:
  mse: 0.0845
  r2: 0.8916

Best model architecture:
GradientBoostingRegressor(learning_rate=0.020993680984793886, max_depth=10,
                          min_samples_leaf=9, min_samples_split=19,
                          n_estimators=193, subsample=0.9043349706436498)