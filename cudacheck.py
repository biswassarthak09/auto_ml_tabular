"""
CUDA Diagnostic Script for AutoML Pipeline
Run this on your NVIDIA machine to diagnose GPU detection issues
"""

import sys
import os

print("üîç CUDA Diagnostic Report")
print("=" * 50)

# 1. Check PyTorch installation and CUDA availability
print("\n1. PyTorch Installation Check:")
try:
    import torch
    print(f"‚úÖ PyTorch version: {torch._version_}")
    print(f"‚úÖ PyTorch CUDA compiled: {torch.version.cuda}")
    print(f"‚úÖ PyTorch CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"‚úÖ CUDA device count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   Device {i}: {torch.cuda.get_device_name(i)}")
        print(f"‚úÖ Current CUDA device: {torch.cuda.current_device()}")
        print(f"‚úÖ CUDA capability: {torch.cuda.get_device_capability()}")
    else:
        print("‚ùå CUDA not available in PyTorch")
except ImportError:
    print("‚ùå PyTorch not installed")
except Exception as e:
    print(f"‚ùå PyTorch error: {e}")

# 2. Check NVIDIA drivers and CUDA runtime
print("\n2. System CUDA Check:")
try:
    import subprocess
    
    # Check nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("‚úÖ nvidia-smi available")
            lines = result.stdout.split('\n')
            for line in lines:
                if 'CUDA Version' in line:
                    print(f"‚úÖ {line.strip()}")
                    break
        else:
            print("‚ùå nvidia-smi failed")
    except FileNotFoundError:
        print("‚ùå nvidia-smi not found")
    except Exception as e:
        print(f"‚ùå nvidia-smi error: {e}")
        
    # Check nvcc
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            lines = result.stdout.split('\n')
            for line in lines:
                if 'release' in line.lower():
                    print(f"‚úÖ NVCC: {line.strip()}")
                    break
        else:
            print("‚ùå nvcc not available")
    except FileNotFoundError:
        print("‚ùå nvcc not found")
    except Exception as e:
        print(f"‚ùå nvcc error: {e}")
        
except Exception as e:
    print(f"‚ùå System check error: {e}")

# 3. Test device creation
print("\n3. Device Creation Test:")
try:
    import torch
    
    # Test auto detection
    auto_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"‚úÖ Auto device: {auto_device}")
    
    # Test explicit CUDA
    if torch.cuda.is_available():
        cuda_device = torch.device('cuda')
        print(f"‚úÖ Explicit CUDA device: {cuda_device}")
        
        # Test tensor operations
        test_tensor = torch.randn(10, 10).to(cuda_device)
        print(f"‚úÖ CUDA tensor created: {test_tensor.device}")
        
        # Test computation
        result = torch.mm(test_tensor, test_tensor.t())
        print(f"‚úÖ CUDA computation successful: {result.device}")
    else:
        print("‚ùå Cannot test CUDA - not available")
        
except Exception as e:
    print(f"‚ùå Device test error: {e}")

# 4. Test AutoML components
print("\n4. AutoML Components CUDA Test:")
try:
    # Test Meta-Learning
    sys.path.append('/Users/sarthakbiswas/Documents/automl/auto_ml_tabular')
    from src.automl_pipeline.meta_learning import AdvancedMetaLearningAutoML
    
    meta_model = AdvancedMetaLearningAutoML('test_cuda_check')
    print(f"‚úÖ Meta-learning device: {meta_model.device}")
    
    # Test NAS-HPO
    from src.automl_pipeline.nas_hpo_optuna import NASHPOOptimizer, PyTorchMLP
    
    nas_optimizer = NASHPOOptimizer('data', 'test_cuda_check')
    print(f"‚úÖ NAS-HPO CUDA available: {nas_optimizer.cuda_available}")
    
    # Test PyTorch MLP
    mlp = PyTorchMLP(device='auto')
    print(f"‚úÖ PyTorch MLP device: {mlp.device}")
    
except Exception as e:
    print(f"‚ùå AutoML components error: {e}")
    import traceback
    traceback.print_exc()

# 5. Environment variables
print("\n5. Environment Variables:")
cuda_vars = ['CUDA_HOME', 'CUDA_PATH', 'CUDA_VISIBLE_DEVICES', 'NVIDIA_VISIBLE_DEVICES']
for var in cuda_vars:
    value = os.environ.get(var, 'Not set')
    print(f"   {var}: {value}")

print("\n" + "=" * 50)
print("üéØ Diagnosis Complete!")
print("\nüí° If CUDA is available but AutoML uses CPU:")
print("   1. Check if PyTorch was installed with CUDA support")
print("   2. Verify CUDA versions match (PyTorch CUDA vs System CUDA)")
print("   3. Check CUDA_VISIBLE_DEVICES environment variable")
print("   4. Try: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")