=== NAS-HPO Optimization Summary ===


Dataset: bike_sharing_demand
--------------------------------------------------
1. mlp: 0.5925 (R²: 0.5925, MSE: 13328.0047, MAE: 79.4100)
2. knn: 0.5772 (R²: 0.5772, MSE: 13822.9235, MAE: 84.4158)
3. random_forest: 0.4143 (R²: 0.4143, MSE: 19150.3509, MAE: 100.6384)
4. xgboost: 0.2652 (R²: 0.2652, MSE: 24066.2535, MAE: 115.4247)

Best Algorithm: mlp
Best Composite Score: 0.5925
Best R²: 0.5925
Best MSE: 13328.0047
Best MAE: 79.4100
Best Params: {
  "n_layers": 2,
  "layer_0_size": 193,
  "layer_1_size": 322,
  "activation": "tanh",
  "learning_rate": 0.0019249173372213357,
  "batch_size": 64,
  "max_iter": 102,
  "alpha": 0.0008989659288459128
}

Dataset: brazilian_houses
--------------------------------------------------
1. mlp: 0.8709 (R²: 0.8709, MSE: 0.0817, MAE: 0.1887)
2. knn: 0.8471 (R²: 0.8471, MSE: 0.0982, MAE: 0.1870)
3. xgboost: 0.7366 (R²: 0.7366, MSE: 0.1660, MAE: 0.2814)
4. random_forest: 0.7083 (R²: 0.7083, MSE: 0.1807, MAE: 0.2707)

Best Algorithm: mlp
Best Composite Score: 0.8709
Best R²: 0.8709
Best MSE: 0.0817
Best MAE: 0.1887
Best Params: {
  "n_layers": 4,
  "layer_0_size": 327,
  "layer_1_size": 402,
  "layer_2_size": 346,
  "layer_3_size": 511,
  "activation": "relu",
  "learning_rate": 0.003185861191769896,
  "batch_size": 32,
  "max_iter": 166,
  "alpha": 0.0009700843195567639
}

Dataset: exam_dataset
--------------------------------------------------
1. random_forest: 0.0000
2. knn: 0.0000
3. xgboost: 0.0000
4. mlp: 0.0000

Best Algorithm: random_forest
Best Composite Score: 0.0000
Best Params: {
  "n_estimators": 218,
  "max_depth": 20,
  "min_samples_split": 15,
  "min_samples_leaf": 6,
  "max_features": "sqrt"
}

Dataset: superconductivity
--------------------------------------------------
1. knn: 0.8852 (R²: 0.8852, MSE: 132.5660, MAE: 6.4555)
2. mlp: 0.8766 (R²: 0.8766, MSE: 142.6997, MAE: 7.3703)
3. random_forest: 0.8107 (R²: 0.8107, MSE: 218.8221, MAE: 9.8353)
4. xgboost: 0.7314 (R²: 0.7314, MSE: 310.6212, MAE: 10.8499)

Best Algorithm: knn
Best Composite Score: 0.8852
Best R²: 0.8852
Best MSE: 132.5660
Best MAE: 6.4555
Best Params: {
  "n_neighbors": 3,
  "weights": "distance",
  "algorithm": "ball_tree",
  "p": 1
}

Dataset: wine_quality
--------------------------------------------------
1. xgboost: 0.3755 (R²: 0.3755, MSE: 0.4824, MAE: 0.5207)
2. random_forest: 0.3661 (R²: 0.3661, MSE: 0.4897, MAE: 0.5378)
3. knn: 0.3630 (R²: 0.3630, MSE: 0.4922, MAE: 0.5376)
4. mlp: 0.2998 (R²: 0.2998, MSE: 0.5410, MAE: 0.5701)

Best Algorithm: xgboost
Best Composite Score: 0.3755
Best R²: 0.3755
Best MSE: 0.4824
Best MAE: 0.5207
Best Params: {
  "n_estimators": 339,
  "max_depth": 13,
  "learning_rate": 0.05412808347071988,
  "subsample": 0.7741910539154085,
  "colsample_bytree": 0.7822766134008259,
  "min_child_weight": 6,
  "reg_alpha": 0.4112243406932234,
  "reg_lambda": 5.365210551673116
}

Dataset: yprop_4_1
--------------------------------------------------
1. random_forest: 0.0563 (R²: 0.0563, MSE: 0.0007, MAE: 0.0192)
2. xgboost: 0.0352 (R²: 0.0352, MSE: 0.0007, MAE: 0.0192)
3. mlp: -0.0156 (R²: -0.0156, MSE: 0.0007, MAE: 0.0204)
4. knn: -0.0308 (R²: -0.0308, MSE: 0.0008, MAE: 0.0200)

Best Algorithm: random_forest
Best Composite Score: 0.0563
Best R²: 0.0563
Best MSE: 0.0007
Best MAE: 0.0192
Best Params: {
  "n_estimators": 164,
  "max_depth": 20,
  "min_samples_split": 2,
  "min_samples_leaf": 2,
  "max_features": "log2"
}